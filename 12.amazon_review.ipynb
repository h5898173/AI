{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQo_HW8XyjAc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **載入預設的庫(20200507MUST)**\n",
        "\n",
        "*   處理bz2\n",
        "*   處理kv值\n",
        "*   載入正規表示法\n",
        "*   載入英文分詞庫\n",
        "*   numpy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmObk426yili",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "d88c10cd-298e-4e83-8ade-dfaa9c4b4fc2"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ttq76ZyzByF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "a0769ddd-d3e6-4a45-beee-c3a6ef573aca"
      },
      "source": [
        "import bz2\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "\n",
        "train_file = bz2.BZ2File('/content/drive/My Drive/train.ft.txt.bz2')\n",
        "test_file = bz2.BZ2File('/content/drive/My Drive/test.ft.txt.bz2')\n",
        "\n",
        "train_file = train_file.readlines()\n",
        "test_file = test_file.readlines()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b0171a2f5ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/train.ft.txt.bz2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/test.ft.txt.bz2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/train.ft.txt.bz2'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbTeJK4jzeUV",
        "colab_type": "text"
      },
      "source": [
        "# **看一下檔案中有什麼，並且看一共有多大**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgoEVM6zuOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_file[67])\n",
        "print(len(train_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaD0vbGC3WKL",
        "colab_type": "text"
      },
      "source": [
        "# **原檔案太大，只取其中80萬，原來有360萬**\n",
        "# **同時將檔案轉成utf8格式**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_VDkph3-Xcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGcyTbh93dh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
        "num_test = 200000  # Using 200,000 reviews from test set\n",
        "# num_train = len(train_file)\n",
        "# num_test = len(test_file)\n",
        "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
        "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDhlx2N93f9T",
        "colab_type": "text"
      },
      "source": [
        "# **看一下檔案中有什麼，並且看一共有多大**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMhmkXYK3pJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_file[67])\n",
        "print(len(train_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0OZyoHm3qyM",
        "colab_type": "text"
      },
      "source": [
        "# **將標記抓出，正評為1、負評為0，同時對訓練集和測試集進行**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pUP83b_3uqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
        "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
        "\n",
        "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
        "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxgMUGDq3vJq",
        "colab_type": "text"
      },
      "source": [
        "# **看一下標記和資料的長相**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HjGnOA3xzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(len(train_labels))\n",
        "print(train_labels[67])\n",
        "print(train_sentences[67])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9LIPds030Qr",
        "colab_type": "text"
      },
      "source": [
        "# **把數字全部變成0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC-SvFes4ABl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some simple cleaning of data\n",
        "for i in range(len(train_sentences)):\n",
        "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoL_ZwTe4Dfz",
        "colab_type": "text"
      },
      "source": [
        "# **找一筆資料出來看**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWr43PKM4G3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_sentences[67])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKEHfagB4IIc",
        "colab_type": "text"
      },
      "source": [
        "# **把網址全部換成**'\\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZmX0wOU4OgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modify URLs to <url>\n",
        "for i in range(len(train_sentences)):\n",
        "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
        "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
        "        \n",
        "for i in range(len(test_sentences)):\n",
        "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
        "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOq7twp14Nf8",
        "colab_type": "text"
      },
      "source": [
        "# **找一筆出來看**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axA3WXXg4U0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with_url = 0\n",
        "for ii, s in enumerate(train_sentences):\n",
        "    if '<url>' in s:\n",
        "        with_url = ii\n",
        "        print(ii)\n",
        "        break\n",
        "print(train_file[5])\n",
        "print(train_sentences[with_url])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPdS5qzB4VUc",
        "colab_type": "text"
      },
      "source": [
        "# **將句子分詞為單字**\n",
        "\n",
        "\n",
        "\n",
        "*   啟用Counter\n",
        "\n",
        "*   先將某條句定清空，再將分詞完之後的單詞存入該句子，使用ntlk\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnsqJujT4bUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "    # The sentences will be stored as a list of words/tokens\n",
        "    train_sentences[i] = []\n",
        "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
        "        words.update([word.lower()])  # Converting all the words to lowercase\n",
        "        train_sentences[i].append(word)\n",
        "    if i%20000 == 0:\n",
        "        print(str((i*100)/num_train) + \"% done\")\n",
        "print(\"100% done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxzekPiH4k8H",
        "colab_type": "text"
      },
      "source": [
        "# **查看分完詞後的句子長相**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u6uOR7R4nzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_sentences[67])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzpiLRqR4pKE",
        "colab_type": "text"
      },
      "source": [
        "# **把只出現一次的字刪除，只留出現兩次以上的字**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuHYT59u4rpe",
        "colab_type": "text"
      },
      "source": [
        "# **把字照出現次數排序**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqnaZem24upF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = {k:v for k,v in words.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "words = sorted(words, key=words.get, reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy58TvML4x1k",
        "colab_type": "text"
      },
      "source": [
        "# **印出一個字來看長什麼樣子**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0if5Obee483V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(len(words))\n",
        "print(words[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-2zQA-_4-1W",
        "colab_type": "text"
      },
      "source": [
        "# **將未知字和補0算一個字加進去**\n",
        "\n",
        "\n",
        "*   開始將字編號\n",
        "\n",
        "*   建立字對編號的關係，雙向所以有兩個\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxy7LHE25HYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "words = ['_PAD','_UNK'] + words\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OURkAN6S5LKd",
        "colab_type": "text"
      },
      "source": [
        "# **印出某個字來看其索引，並且印出索引看字是什麼**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fges1cZ5O29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word2idx['the'])\n",
        "print(idx2word[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7BYLJS_5QQP",
        "colab_type": "text"
      },
      "source": [
        "# **把訓練句子中的所有文字都轉成索引，還有測試句子**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mjih3fg5SQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, sentence in enumerate(train_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    # For test sentences, we have to tokenize the sentences as well\n",
        "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EDTvOyK5Sv_",
        "colab_type": "text"
      },
      "source": [
        "# **印出一個句子出來看看**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN7Tu3Kb5Uic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_sentences[67]))\n",
        "print(train_sentences[67])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6m09cqX5V9A",
        "colab_type": "text"
      },
      "source": [
        "# **印一個超過200的句子來看**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnbElQ6f5Ym2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "more_than_200 = 0\n",
        "for ii, s in enumerate(train_sentences):\n",
        "    if len(s) > 200:\n",
        "        more_than_200 = ii\n",
        "        print(len(s))\n",
        "        print(ii)\n",
        "        break\n",
        "print(train_file[more_than_200])\n",
        "print(train_sentences[more_than_200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuB4iXtG5aDN",
        "colab_type": "text"
      },
      "source": [
        "# **把句子不到200的前面補上0，如果超過200，就把200以後的刪除，讓每個句子的長度都是200**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n98QGrYL5dED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "\n",
        "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
        "\n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "test_sentences = pad_input(test_sentences, seq_len)\n",
        "\n",
        "# Converting our labels into numpy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxdd9nT-5eZj",
        "colab_type": "text"
      },
      "source": [
        "# **印一個補過0的出來看**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdwxAmrC5hz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_sentences[67]))\n",
        "print(train_sentences[67])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl_4aPdv5iOG",
        "colab_type": "text"
      },
      "source": [
        "# **印出一個被截斷的來看¶**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFPBMmd15mIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_sentences[more_than_200]))\n",
        "print(train_sentences[more_than_200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvOacK875lv5",
        "colab_type": "text"
      },
      "source": [
        "# **把測試資料集分成test和validation**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwaTph3q5s5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.5 # 50% validation, 50% test\n",
        "split_id = int(split_frac * len(test_sentences))\n",
        "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
        "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIgvJPU95u_G",
        "colab_type": "text"
      },
      "source": [
        "# **看測試資料集的長度為多少**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmxlCFmF5yNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(val_sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyrmIrCi5yk1",
        "colab_type": "text"
      },
      "source": [
        "# **將資料載入pytorch格式**\n",
        "\n",
        "\n",
        "*   三個資料集的載入\n",
        "*   設定批次大小\n",
        "*   建立dataloader的generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLgR12Xz56wN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
        "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maJ3G6k057O-",
        "colab_type": "text"
      },
      "source": [
        "# **檢查是否有gpu存在**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZgcJSBO59O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIwbXYyB5-0V",
        "colab_type": "text"
      },
      "source": [
        "# **建立網路**\n",
        "\n",
        "*   vocab_size：字彙檔的大小，就是一個one-hot的資料\n",
        "*   output_size：大小為1，因為只要一個值，介於0和1之間\n",
        "*   embedding_dim：就是word2vec的特徵大小，從vocab_size降維過來的\n",
        "*   hidden_dim：hidden和cell states的維度\n",
        "*   n_layers：lstm的層數\n",
        "*   drop_prob：dropout的比例\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HugSzYBT6LHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC4jWDBi6M92",
        "colab_type": "text"
      },
      "source": [
        "# **定義超參數**\n",
        "\n",
        "\n",
        "*   定義超參數\n",
        "*   建立模型\n",
        "*   將模型移入gpu中\n",
        "*   定義學習率\n",
        "*   定義損失函數\n",
        "*   定義最佳化方法\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7jlxjKY6YCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "\n",
        "lr=0.005\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh5nqXeT6aaG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **接下來把整個網路手動走一遍**\n",
        "# **啟始化hidden和cell states的值**\n",
        "\n",
        "\n",
        "*  由於model已經移入gpu，因此呼叫model的函數產生的值也會在gpu中，就會出現cuda device\n",
        "*  另外要建立tuple，每一個批次都要有自己的tuple，不是一個大tuple丟去就好了\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFdcQfm_6g_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hh = model.init_hidden(batch_size)\n",
        "hh = tuple([e.data for e in hh])\n",
        "print(hh[1].shape)\n",
        "print(hh[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwdzC0HZ6nXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = next(iter(train_loader)) # 取一段training data出來\n",
        "x = inputs[0].to(device) #把資料移入gpu\n",
        "batch_num = x.size(0) #取得批次大小，256\n",
        "x = x.long() # 把輸入值轉成長整數\n",
        "print(x.shape) # 看看x的型狀長什麼樣子\n",
        "print(x) #印出一個x來看看"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmV3Uq-X6otZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, embedding_dim) # 把x從one-hot的詞彙大小轉成word2vec的大小\n",
        "embedding = embedding.to(device) #將這個函數放入gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aedv_EBq6qFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "embeds = embedding(x) # 將詞彙轉成wordvec\n",
        "embeds.shape #看看型狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGst2HJu6sYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=0.5, batch_first=True) # 建立一個lstm\n",
        "lstm = lstm.to(device) #將lstm移入gpu\n",
        "lstm_out, hidden = lstm(embeds, hh) #讓剛才的資料跑過lstm，產生輸出值及新的hidden值"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHzVK7Aw6uCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lstm_out.shape) #看看從lstm跑出來的東西型狀\n",
        "print(hidden[0].shape) # 看看hidden第一個值的形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE3bASL96vIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_out = lstm_out.contiguous().view(-1, hidden_dim) #把lstm的輸出值攤平\n",
        "lstm_out.shape #看看形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMqBOAWn6wlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = nn.Dropout(0.5) #設定dropout\n",
        "out = dropout(lstm_out) # 讓輸出值走一次dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iSopjBH6xyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(out.shape) # 查看輸出值的形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZVr5VN36y--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc = nn.Linear(hidden_dim, 1) #定義最後一層fc\n",
        "fc = fc.to(device) #移入gpu\n",
        "out = fc(out) #讓輸入出值走一次fc\n",
        "print(out.shape) # 看一下形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Eflwwg60h1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigmoid = nn.Sigmoid() # 讓輸出值介於0和1之間\n",
        "sigmoid = sigmoid.to(device) #將sigmoid移入gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XOp_GHI60n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = sigmoid(out) #讓輸出值走一次sigmoid\n",
        "print(out.shape) #看看形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuzbFJl064ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = out.view(batch_size, -1) #把輸出值還原成(批次大小，句子長度)\n",
        "print(out.shape) # 看形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2ljIy2E64nQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = out[:,-1] #取出最後結果值\n",
        "print(out.shape)# 看形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvXtDC4t69SS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(out) # 看每批256個輸出值"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBzcoxgi6_mB",
        "colab_type": "text"
      },
      "source": [
        "# **接下來進行真正訓練**\n",
        "\n",
        "\n",
        "*   輪數為2\n",
        "*   計數值\n",
        "*   每1000步就印出結果\n",
        "*   一開始的loss值為無限大\n",
        "# **開始訓練**\n",
        "\n",
        "\n",
        "\n",
        "*   初始化hidden值\n",
        "*   走訓練流程\n",
        "*  每1000步就印出結果一次\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBfJVcag7R2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2\n",
        "counter = 0\n",
        "print_every = 1000\n",
        "clip = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        h = tuple([e.data for e in h])\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if counter%print_every == 0:\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inp, lab in val_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                inp, lab = inp.to(device), lab.to(device)\n",
        "                out, val_h = model(inp, val_h)\n",
        "                val_loss = criterion(out.squeeze(), lab.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            if np.mean(val_losses) <= valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrIYLHQU7TXM",
        "colab_type": "text"
      },
      "source": [
        "# **進行模型結果預測**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fokydrw7VGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))\n",
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}